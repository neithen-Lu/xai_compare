{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distill-BERT pretrained on SQUaD\n",
    "\n",
    "Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 22:21:49.576180: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-27 22:21:49.578826: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-27 22:21:49.613574: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-27 22:21:49.613609: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-27 22:21:49.613632: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-27 22:21:49.620301: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-27 22:21:49.621217: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-27 22:21:50.381996: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-27 22:21:54,154\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "from typing import Optional, Union, List, Dict, Tuple\n",
    "from IPython.display import HTML\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "from transformers.optimization_tf import WarmUp\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer, PreTrainedTokenizer\n",
    "\n",
    "from alibi.explainers import IntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 22:22:02.027378: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "auto_model_distilbert = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sentence(x: List[int], reverse_index: Dict[int, str], unk_token: str = '[UNK]') -> str:\n",
    "    \"\"\"\n",
    "    Decodes the tokenized sentences from keras IMDB dataset into plain text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x\n",
    "        List of integers to be docoded.\n",
    "    revese_index\n",
    "        Reverse index map, from `int` to `str`.\n",
    "    unk_token\n",
    "        Unkown token to be used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Decoded sentence.\n",
    "    \"\"\"\n",
    "    # the `-3` offset is due to the special tokens used by keras\n",
    "    # see https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset\n",
    "    return \" \".join([reverse_index.get(i - 3, unk_token) for i in x])\n",
    "\n",
    "\n",
    "def process_sentences(sentence: List[str],\n",
    "                      tokenizer: PreTrainedTokenizer,\n",
    "                      max_len: int) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Tokenize the text sentences.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence\n",
    "        Sentence to be processed.\n",
    "    tokenizer\n",
    "        Tokenizer to be used.\n",
    "    max_len\n",
    "        Controls the maximum length to use by one of the truncation/padding parameters.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tokenized representation containing:\n",
    "     - input_ids\n",
    "     - attention_mask\n",
    "    \"\"\"\n",
    "    # since we are using the model for classification, we need to include special char (i.e, '[CLS]', ''[SEP]')\n",
    "    # check the example here: https://huggingface.co/transformers/v4.4.2/quicktour.html\n",
    "    z = tokenizer(sentence,\n",
    "                  add_special_tokens=True,\n",
    "                  padding='max_length',\n",
    "                  max_length=max_len,\n",
    "                  truncation=True,\n",
    "                  return_attention_mask = True,\n",
    "                  return_tensors='np')\n",
    "    return z\n",
    "\n",
    "def process_input(sentence: List[str],\n",
    "                  tokenizer: PreTrainedTokenizer,\n",
    "                  max_len: int) -> Tuple[np.ndarray, dict]:\n",
    "    \"\"\"\n",
    "    Preprocess input sentence befor sending to transformer model.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    sentence\n",
    "        Sentence to be processed.\n",
    "    tokenizer\n",
    "        Tokenizer to be used.\n",
    "    max_len\n",
    "        Controls the maximum length to use by one of the truncation/padding parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple consisting of the input_ids and a dictionary contaning the attention_mask.\n",
    "    \"\"\"\n",
    "    # tokenize the sentences using the transformer's tokenizer.\n",
    "    tokenized_samples = process_sentences(sentence, tokenizer, max_len)\n",
    "    X_test = tokenized_samples['input_ids'].astype(np.int32)\n",
    "\n",
    "    # the values of the kwargs have to be `tf.Tensor`.\n",
    "    # see transformers issue #14404: https://github.com/huggingface/transformers/issues/14404\n",
    "    # solved from v4.16.0\n",
    "    kwargs = {k: tf.constant(v) for k, v in tokenized_samples.items() if k != 'input_ids'}\n",
    "    return X_test, kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoModelWrapper(keras.Model):\n",
    "    def __init__(self, transformer: keras.Model, **kwargs):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        transformer\n",
    "            Transformer to be wrapped.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def call(self,\n",
    "             input_ids: Union[np.ndarray, tf.Tensor],\n",
    "             attention_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
    "             training: bool = False):\n",
    "        \"\"\"\n",
    "        Performs forward pass throguh the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids\n",
    "            Indices of input sequence tokens in the vocabulary.\n",
    "        attention_mask\n",
    "            Mask to avoid performing attention on padding token indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Classification probabilities.\n",
    "        \"\"\"\n",
    "        out = self.transformer(input_ids=input_ids, attention_mask=attention_mask, training=training)\n",
    "        return tf.nn.softmax(out.logits, axis=-1)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "auto_model = AutoModelWrapper(auto_model_distilbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layer not in the list of `model.layers`. Passing the layer directly would not permit the serialization of the explainer. This is due to nested layers. To permit the serialization of the explainer, provide the layer as a callable which returns the layer given the model.\n"
     ]
    }
   ],
   "source": [
    "n_steps = 50\n",
    "internal_batch_size = 5\n",
    "method = \"gausslegendre\"\n",
    "\n",
    "# define Integrated Gradients explainer\n",
    "ig  = IntegratedGradients(auto_model,\n",
    "                          layer=auto_model.layers[0].layers[0].embeddings,\n",
    "                          n_steps=n_steps,\n",
    "                          method=method,\n",
    "                          internal_batch_size=internal_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some text to be explained\n",
    "text_samples = [\n",
    "    \"What a great movie!\",\n",
    "    \"The Interview was neither that funny nor that witty. \"\n",
    "    \"Even if there are words like funny and witty, the overall structure is a negative type.\"\n",
    "]\n",
    "\n",
    "# process input to be explained\n",
    "X_test, kwargs = process_input(sentence=text_samples,\n",
    "                               tokenizer=tokenizer,\n",
    "                               max_len=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "predictions = auto_model(X_test, **kwargs).numpy().argmax(axis=1)\n",
    "\n",
    "# get the baseline\n",
    "mask = np.isin(X_test, tokenizer.all_special_ids)\n",
    "baselines = X_test * mask + tokenizer.pad_token_id * (1 - mask)\n",
    "\n",
    "# get explanation\n",
    "explanation = ig.explain(X_test,\n",
    "                         forward_kwargs=kwargs,\n",
    "                         baselines=baselines,\n",
    "                         target=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  hlstr(string: str , color: str = 'white') -> str:\n",
    "    \"\"\"\n",
    "    Return HTML markup highlighting text with the desired color.\n",
    "    \"\"\"\n",
    "    return f\"<mark style=background-color:{color}>{string} </mark>\"\n",
    "\n",
    "\n",
    "def colorize(attrs: np.ndarray, cmap: str = 'PiYG') -> List:\n",
    "    \"\"\"\n",
    "    Compute hex colors based on the attributions for a single instance.\n",
    "    Uses a diverging colorscale by default and normalizes and scales\n",
    "    the colormap so that colors are consistent with the attributions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    attrs\n",
    "        Attributions to be visualized.\n",
    "    cmap\n",
    "        Matplotlib cmap type.\n",
    "    \"\"\"\n",
    "    cmap_bound = np.abs(attrs).max()\n",
    "    norm = mpl.colors.Normalize(vmin=-cmap_bound, vmax=cmap_bound)\n",
    "    cmap = mpl.cm.get_cmap(cmap)\n",
    "    return list(map(lambda x: mpl.colors.rgb2hex(cmap(norm(x))), attrs))\n",
    "\n",
    "\n",
    "def display(X: np.ndarray,\n",
    "            attrs: np.ndarray,\n",
    "            tokenizer: PreTrainedTokenizer,\n",
    "            pred: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Display the attribution of a given instance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X\n",
    "        Instance to display the attributions for.\n",
    "    attrs\n",
    "        Attributions values for the given instance.\n",
    "    tokenizer\n",
    "        Tokenizer to be used for decoding.\n",
    "    pred\n",
    "        Classification label (prediction) for the given instance.\n",
    "    \"\"\"\n",
    "    pred_dict = {1: 'Positive review', 0: 'Negative review'}\n",
    "\n",
    "    # remove padding\n",
    "    fst_pad_indices = np.where(X ==tokenizer.pad_token_id)[0]\n",
    "    if len(fst_pad_indices) > 0:\n",
    "        X, attrs = X[:fst_pad_indices[0]], attrs[:fst_pad_indices[0]]\n",
    "\n",
    "    # decode tokens and get colors\n",
    "    tokens = [tokenizer.decode([X[i]]) for i in range(len(X))]\n",
    "    colors = colorize(attrs)\n",
    "\n",
    "    print(f'Predicted label =  {pred}: {pred_dict[pred]}')\n",
    "    return HTML(\"\".join(list(map(hlstr, tokens, colors))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label =  0: Negative review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<mark style=background-color:#f7f7f6>[CLS] </mark><mark style=background-color:#fbe8f2>the </mark><mark style=background-color:#c7e89f>interview </mark><mark style=background-color:#faebf3>was </mark><mark style=background-color:#7dba40>neither </mark><mark style=background-color:#f6f7f5>that </mark><mark style=background-color:#f8f5f6>funny </mark><mark style=background-color:#589b28>nor </mark><mark style=background-color:#f1f6ea>that </mark><mark style=background-color:#f3bcdd>witty </mark><mark style=background-color:#e4f4cd>. </mark><mark style=background-color:#d4edb3>even </mark><mark style=background-color:#edf6df>if </mark><mark style=background-color:#f0f6e7>there </mark><mark style=background-color:#f1f6ea>are </mark><mark style=background-color:#f3f7ef>words </mark><mark style=background-color:#98cc5f>like </mark><mark style=background-color:#f8f4f6>funny </mark><mark style=background-color:#ebf6db>and </mark><mark style=background-color:#efb0d6>witty </mark><mark style=background-color:#cdeaa7>, </mark><mark style=background-color:#f2f6ec>the </mark><mark style=background-color:#4b8f21>overall </mark><mark style=background-color:#5a9d29>structure </mark><mark style=background-color:#7dba40>is </mark><mark style=background-color:#98cc5f>a </mark><mark style=background-color:#276419>negative </mark><mark style=background-color:#b9e187>type </mark><mark style=background-color:#bbe28a>. </mark><mark style=background-color:#f7f7f6>[SEP] </mark>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get attributions values from the explanation object\n",
    "attrs = explanation.attributions[0]\n",
    "attrs = attrs.sum(axis=2)\n",
    "index = 1\n",
    "display(X=X_test[index], attrs=attrs[index], pred=predictions[index], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.AutoModelWrapper at 0x7fbcd452f310>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from en-core-web-md==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.7.4)\n",
      "Requirement already satisfied: jinja2 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (70.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.6.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.14.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.2)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, model symlinks are not supported anymore. You can\n",
      "load trained pipeline packages using their full names or from a directory\n",
      "path.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from alibi.utils import spacy_model\n",
    "\n",
    "model = 'en_core_web_md'\n",
    "spacy_model(model=model)\n",
    "nlp = spacy.load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n"
     ]
    }
   ],
   "source": [
    "from alibi.explainers.anchors.anchor_text import AnchorText\n",
    "import transformers\n",
    "model = transformers.pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    return_all_scores=True\n",
    ")\n",
    "postprocess = lambda outputs: np.array([[s[\"score\"] for s in ss] for ss in outputs])\n",
    "def predict(x):\n",
    "    return postprocess(model(x))\n",
    "explainer = AnchorText(predictor=predict, sampling_strategy=\"unknown\",\n",
    "                       nlp=nlp,                           # spacy object\n",
    "                        sample_proba=0.5)                 # probability of a word to be replaced by UNK token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nor']\n"
     ]
    }
   ],
   "source": [
    "explanation = explainer.explain(text_samples[1], threshold=0.95)\n",
    "print(explanation.anchor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnixai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
