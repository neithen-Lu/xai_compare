{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distill-BERT pretrained on SQUaD\n",
    "\n",
    "Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 16:03:52.388988: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-18 16:03:52.393337: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-18 16:03:52.444366: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-18 16:03:52.444418: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-18 16:03:52.444463: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-18 16:03:52.455375: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-18 16:03:52.458327: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-18 16:03:53.582583: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/kxlu/anaconda3/envs/alibi/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-18 16:03:59,039\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Union, List, Dict, Tuple\n",
    "from IPython.display import HTML\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "from transformers.optimization_tf import WarmUp\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer, PreTrainedTokenizer\n",
    "\n",
    "from alibi.explainers import IntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 16:04:00.200470: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "auto_model_distilbert = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sentence(x: List[int], reverse_index: Dict[int, str], unk_token: str = '[UNK]') -> str:\n",
    "    \"\"\"\n",
    "    Decodes the tokenized sentences from keras IMDB dataset into plain text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x\n",
    "        List of integers to be docoded.\n",
    "    revese_index\n",
    "        Reverse index map, from `int` to `str`.\n",
    "    unk_token\n",
    "        Unkown token to be used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Decoded sentence.\n",
    "    \"\"\"\n",
    "    # the `-3` offset is due to the special tokens used by keras\n",
    "    # see https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset\n",
    "    return \" \".join([reverse_index.get(i - 3, unk_token) for i in x])\n",
    "\n",
    "\n",
    "def process_sentences(sentence: List[str],\n",
    "                      tokenizer: PreTrainedTokenizer,\n",
    "                      max_len: int) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Tokenize the text sentences.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence\n",
    "        Sentence to be processed.\n",
    "    tokenizer\n",
    "        Tokenizer to be used.\n",
    "    max_len\n",
    "        Controls the maximum length to use by one of the truncation/padding parameters.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tokenized representation containing:\n",
    "     - input_ids\n",
    "     - attention_mask\n",
    "    \"\"\"\n",
    "    # since we are using the model for classification, we need to include special char (i.e, '[CLS]', ''[SEP]')\n",
    "    # check the example here: https://huggingface.co/transformers/v4.4.2/quicktour.html\n",
    "    z = tokenizer(sentence,\n",
    "                  add_special_tokens=True,\n",
    "                  padding='max_length',\n",
    "                  max_length=max_len,\n",
    "                  truncation=True,\n",
    "                  return_attention_mask = True,\n",
    "                  return_tensors='np')\n",
    "    return z\n",
    "\n",
    "def process_input(sentence: List[str],\n",
    "                  tokenizer: PreTrainedTokenizer,\n",
    "                  max_len: int) -> Tuple[np.ndarray, dict]:\n",
    "    \"\"\"\n",
    "    Preprocess input sentence befor sending to transformer model.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    sentence\n",
    "        Sentence to be processed.\n",
    "    tokenizer\n",
    "        Tokenizer to be used.\n",
    "    max_len\n",
    "        Controls the maximum length to use by one of the truncation/padding parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple consisting of the input_ids and a dictionary contaning the attention_mask.\n",
    "    \"\"\"\n",
    "    # tokenize the sentences using the transformer's tokenizer.\n",
    "    tokenized_samples = process_sentences(sentence, tokenizer, max_len)\n",
    "    X_test = tokenized_samples['input_ids'].astype(np.int32)\n",
    "\n",
    "    # the values of the kwargs have to be `tf.Tensor`.\n",
    "    # see transformers issue #14404: https://github.com/huggingface/transformers/issues/14404\n",
    "    # solved from v4.16.0\n",
    "    kwargs = {k: tf.constant(v) for k, v in tokenized_samples.items() if k != 'input_ids'}\n",
    "    return X_test, kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoModelWrapper(keras.Model):\n",
    "    def __init__(self, transformer: keras.Model, **kwargs):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        transformer\n",
    "            Transformer to be wrapped.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def call(self,\n",
    "             input_ids: Union[np.ndarray, tf.Tensor],\n",
    "             attention_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
    "             training: bool = False):\n",
    "        \"\"\"\n",
    "        Performs forward pass throguh the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids\n",
    "            Indices of input sequence tokens in the vocabulary.\n",
    "        attention_mask\n",
    "            Mask to avoid performing attention on padding token indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Classification probabilities.\n",
    "        \"\"\"\n",
    "        out = self.transformer(input_ids=input_ids, attention_mask=attention_mask, training=training)\n",
    "        return tf.nn.softmax(out.logits, axis=-1)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "auto_model = AutoModelWrapper(auto_model_distilbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layer not in the list of `model.layers`. Passing the layer directly would not permit the serialization of the explainer. This is due to nested layers. To permit the serialization of the explainer, provide the layer as a callable which returns the layer given the model.\n"
     ]
    }
   ],
   "source": [
    "n_steps = 50\n",
    "internal_batch_size = 5\n",
    "method = \"gausslegendre\"\n",
    "\n",
    "# define Integrated Gradients explainer\n",
    "ig  = IntegratedGradients(auto_model,\n",
    "                          layer=auto_model.layers[0].layers[0].embeddings,\n",
    "                          n_steps=n_steps,\n",
    "                          method=method,\n",
    "                          internal_batch_size=internal_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some text to be explained\n",
    "text_samples = [\n",
    "    'I love you, I like you',\n",
    "    'I love you, I like you, but I also kind of dislike you',\n",
    "    'Everything is so nice about you'\n",
    "]\n",
    "\n",
    "# process input to be explained\n",
    "X_test, kwargs = process_input(sentence=text_samples,\n",
    "                               tokenizer=tokenizer,\n",
    "                               max_len=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "predictions = auto_model(X_test, **kwargs).numpy().argmax(axis=1)\n",
    "\n",
    "# get the baseline\n",
    "mask = np.isin(X_test, tokenizer.all_special_ids)\n",
    "baselines = X_test * mask + tokenizer.pad_token_id * (1 - mask)\n",
    "\n",
    "# get explanation\n",
    "explanation = ig.explain(X_test,\n",
    "                         forward_kwargs=kwargs,\n",
    "                         baselines=baselines,\n",
    "                         target=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  hlstr(string: str , color: str = 'white') -> str:\n",
    "    \"\"\"\n",
    "    Return HTML markup highlighting text with the desired color.\n",
    "    \"\"\"\n",
    "    return f\"<mark style=background-color:{color}>{string} </mark>\"\n",
    "\n",
    "\n",
    "def colorize(attrs: np.ndarray, cmap: str = 'PiYG') -> List:\n",
    "    \"\"\"\n",
    "    Compute hex colors based on the attributions for a single instance.\n",
    "    Uses a diverging colorscale by default and normalizes and scales\n",
    "    the colormap so that colors are consistent with the attributions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    attrs\n",
    "        Attributions to be visualized.\n",
    "    cmap\n",
    "        Matplotlib cmap type.\n",
    "    \"\"\"\n",
    "    cmap_bound = np.abs(attrs).max()\n",
    "    norm = mpl.colors.Normalize(vmin=-cmap_bound, vmax=cmap_bound)\n",
    "    cmap = mpl.cm.get_cmap(cmap)\n",
    "    return list(map(lambda x: mpl.colors.rgb2hex(cmap(norm(x))), attrs))\n",
    "\n",
    "\n",
    "def display(X: np.ndarray,\n",
    "            attrs: np.ndarray,\n",
    "            tokenizer: PreTrainedTokenizer,\n",
    "            pred: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Display the attribution of a given instance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X\n",
    "        Instance to display the attributions for.\n",
    "    attrs\n",
    "        Attributions values for the given instance.\n",
    "    tokenizer\n",
    "        Tokenizer to be used for decoding.\n",
    "    pred\n",
    "        Classification label (prediction) for the given instance.\n",
    "    \"\"\"\n",
    "    pred_dict = {1: 'Positive review', 0: 'Negative review'}\n",
    "\n",
    "    # remove padding\n",
    "    fst_pad_indices = np.where(X ==tokenizer.pad_token_id)[0]\n",
    "    if len(fst_pad_indices) > 0:\n",
    "        X, attrs = X[:fst_pad_indices[0]], attrs[:fst_pad_indices[0]]\n",
    "\n",
    "    # decode tokens and get colors\n",
    "    tokens = [tokenizer.decode([X[i]]) for i in range(len(X))]\n",
    "    colors = colorize(attrs)\n",
    "\n",
    "    print(f'Predicted label =  {pred}: {pred_dict[pred]}')\n",
    "    return HTML(\"\".join(list(map(hlstr, tokens, colors))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label =  0: Negative review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<mark style=background-color:#f7f7f6>[CLS] </mark><mark style=background-color:#fcdbed>i </mark><mark style=background-color:#e181b5>love </mark><mark style=background-color:#f9d1e8>you </mark><mark style=background-color:#f9eff4>, </mark><mark style=background-color:#f9eef4>i </mark><mark style=background-color:#faeaf2>like </mark><mark style=background-color:#fce5f1>you </mark><mark style=background-color:#f3f6ed>, </mark><mark style=background-color:#c9e8a2>but </mark><mark style=background-color:#f9f1f5>i </mark><mark style=background-color:#f8f2f5>also </mark><mark style=background-color:#edf6e1>kind </mark><mark style=background-color:#e8f5d5>of </mark><mark style=background-color:#276419>dislike </mark><mark style=background-color:#fde0ef>you </mark><mark style=background-color:#f7f7f6>[SEP] </mark>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get attributions values from the explanation object\n",
    "attrs = explanation.attributions[0]\n",
    "attrs = attrs.sum(axis=2)\n",
    "index = 1\n",
    "display(X=X_test[index], attrs=attrs[index], pred=predictions[index], tokenizer=tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnixai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
